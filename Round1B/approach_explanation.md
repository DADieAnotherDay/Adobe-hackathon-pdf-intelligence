# Approach Explanation – Round 1B: Persona-Driven Document Intelligence

## Overview

The solution is designed to intelligently analyze a set of PDF documents and extract the most relevant sections based on a given persona and job-to-be-done. The goal is to simulate how a human reader would scan multiple documents to find sections that are most useful for their objective.

## Methodology

### 1. Input Handling
The system accepts:
- A collection of PDFs (3–10 files)
- A `persona.json` file defining the user's role (persona) and their task (job-to-be-done)

The container reads all PDFs and the persona metadata from the `/app/input` directory.

### 2. Section Extraction
Each PDF is parsed using PyMuPDF to extract content page-by-page. Blocks of text with reasonable length are treated as candidate sections. The first line in each block is assumed to be the section title, and the entire block is retained as the content.

This process works best with text-based PDFs. Image-based PDFs (like scanned handwritten notes) would require OCR, which is not included here due to resource constraints.

### 3. Relevance Ranking
To determine relevance between the job description and each extracted section, the following technique is used:
- The job description is treated as a query.
- TF-IDF (Term Frequency-Inverse Document Frequency) vectors are computed using scikit-learn.
- Cosine similarity is used to compare the job query against each section.
- The top 5 most relevant sections are selected and ranked based on similarity score.

This method allows the system to dynamically prioritize content based on the context provided in the persona definition.

### 4. Subsection Refinement
For each top-ranked section, a simple summary is generated by selecting the first 1–2 sentences. This provides a concise preview of the section content.

### 5. Output Format
The final output is saved to `output/output.json` and includes:
- Metadata (persona, job, timestamp, input documents)
- Top-ranked sections (with title, document, page number, and importance rank)
- Refined text from each section

## Offline Compliance
- The entire solution runs offline and uses lightweight libraries only.
- The Docker image is CPU-only and under the 1GB model size limit.

## Extensibility
The solution is modular and can be improved by:
- Adding OCR for scanned documents
- Replacing TF-IDF with transformer-based embeddings (within size limits)
- Fine-tuning summarization for academic vs. business content

---

This approach balances speed, simplicity, and offline performance, while still delivering meaningful results tailored to user needs.
